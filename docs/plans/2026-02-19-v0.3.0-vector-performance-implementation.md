# Audrey v0.3.0 — Vector Performance Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Replace all JS-side vector operations with native sqlite-vec KNN queries, add batch encoding and streaming recall. Zero JS cosine math after this.

**Architecture:** sqlite-vec extension loaded at DB creation. vec0 virtual tables with cosine distance metric serve as primary vector store. Metadata columns (state, source, consolidated) enable SQL-native filtering in KNN queries. Existing tables keep all non-vector columns. cosineSimilarity removed from JS entirely.

**Tech Stack:** Node.js (ES modules), better-sqlite3, sqlite-vec, vitest

**Design doc:** `docs/plans/2026-02-19-v0.3.0-vector-performance-design.md`

---

## Task 1: sqlite-vec Foundation

**Files:**
- Modify: `src/db.js`
- Create: `tests/vec.test.js`

**Step 1: Write the failing tests**

```js
// tests/vec.test.js
import { describe, it, expect, beforeEach, afterEach } from 'vitest';
import { createDatabase, closeDatabase } from '../src/db.js';
import { existsSync, rmSync, mkdirSync } from 'node:fs';

const TEST_DIR = './test-vec-data';

describe('sqlite-vec integration', () => {
  let db;

  beforeEach(() => {
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
    mkdirSync(TEST_DIR, { recursive: true });
  });

  afterEach(() => {
    if (db) closeDatabase(db);
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
  });

  it('loads sqlite-vec extension and creates vec0 tables when dimensions provided', () => {
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    // Verify vec0 tables exist
    const tables = db.prepare(
      "SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'vec_%'"
    ).all().map(r => r.name);
    expect(tables).toContain('vec_episodes');
    expect(tables).toContain('vec_semantics');
    expect(tables).toContain('vec_procedures');
  });

  it('stores dimensions in config table', () => {
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    const row = db.prepare("SELECT value FROM audrey_config WHERE key = 'dimensions'").get();
    expect(row.value).toBe('8');
  });

  it('validates dimensions match on subsequent opens', () => {
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    closeDatabase(db);
    // Re-open with same dimensions — should work
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    expect(db.open).toBe(true);
  });

  it('throws when dimensions mismatch on re-open', () => {
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    closeDatabase(db);
    db = null;
    expect(() => createDatabase(TEST_DIR, { dimensions: 16 })).toThrow('dimensions mismatch');
  });

  it('skips vec0 tables when no dimensions provided', () => {
    db = createDatabase(TEST_DIR);
    const tables = db.prepare(
      "SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'vec_%'"
    ).all();
    expect(tables.length).toBe(0);
  });

  it('can insert and KNN query a vector', () => {
    db = createDatabase(TEST_DIR, { dimensions: 4 });
    db.prepare("INSERT INTO vec_episodes(id, embedding, source, consolidated) VALUES (?, ?, ?, ?)")
      .run('ep-1', Buffer.from(new Float32Array([1, 0, 0, 0]).buffer), 'direct-observation', 0);
    db.prepare("INSERT INTO vec_episodes(id, embedding, source, consolidated) VALUES (?, ?, ?, ?)")
      .run('ep-2', Buffer.from(new Float32Array([0, 1, 0, 0]).buffer), 'tool-result', 0);

    const query = Buffer.from(new Float32Array([0.9, 0.1, 0, 0]).buffer);
    const results = db.prepare(
      "SELECT id, distance FROM vec_episodes WHERE embedding MATCH ? AND k = 2"
    ).all(query);

    expect(results.length).toBe(2);
    expect(results[0].id).toBe('ep-1'); // closest to [0.9, 0.1, 0, 0]
  });

  it('filters by metadata in KNN query', () => {
    db = createDatabase(TEST_DIR, { dimensions: 4 });
    const vec = Buffer.from(new Float32Array([1, 0, 0, 0]).buffer);
    db.prepare("INSERT INTO vec_episodes(id, embedding, source, consolidated) VALUES (?, ?, ?, ?)")
      .run('ep-1', vec, 'direct-observation', 0);
    db.prepare("INSERT INTO vec_episodes(id, embedding, source, consolidated) VALUES (?, ?, ?, ?)")
      .run('ep-2', vec, 'tool-result', 1); // consolidated

    const results = db.prepare(
      "SELECT id FROM vec_episodes WHERE embedding MATCH ? AND k = 10 AND consolidated = 0"
    ).all(vec);

    expect(results.length).toBe(1);
    expect(results[0].id).toBe('ep-1');
  });
});

describe('migration from v0.2.0', () => {
  let db;

  beforeEach(() => {
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
    mkdirSync(TEST_DIR, { recursive: true });
  });

  afterEach(() => {
    if (db) closeDatabase(db);
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
  });

  it('migrates existing embedding BLOBs to vec0 tables', () => {
    // Create a v0.2.0-style database (no vec0 tables)
    db = createDatabase(TEST_DIR);
    const vec = Buffer.from(new Float32Array([1, 0, 0, 0, 0, 0, 0, 0]).buffer);
    db.prepare(`INSERT INTO episodes (id, content, embedding, source, source_reliability, created_at)
      VALUES (?, ?, ?, ?, ?, ?)`).run('ep-1', 'test', vec, 'direct-observation', 0.95, new Date().toISOString());
    closeDatabase(db);

    // Re-open with dimensions — should migrate
    db = createDatabase(TEST_DIR, { dimensions: 8 });
    const vecRow = db.prepare("SELECT id FROM vec_episodes WHERE id = 'ep-1'").get();
    expect(vecRow).toBeDefined();
    expect(vecRow.id).toBe('ep-1');
  });
});
```

**Step 2: Run tests to verify they fail**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/vec.test.js`
Expected: FAIL — vec0 tables not created, audrey_config not created

**Step 3: Implement sqlite-vec loading in `src/db.js`**

```js
// src/db.js — full replacement
import Database from 'better-sqlite3';
import * as sqliteVec from 'sqlite-vec';
import { join } from 'node:path';
import { mkdirSync } from 'node:fs';

const SCHEMA = `
  CREATE TABLE IF NOT EXISTS episodes (
    id TEXT PRIMARY KEY,
    content TEXT NOT NULL,
    embedding BLOB,
    source TEXT NOT NULL CHECK(source IN ('direct-observation','told-by-user','tool-result','inference','model-generated')),
    source_reliability REAL NOT NULL,
    salience REAL DEFAULT 0.5,
    tags TEXT,
    causal_trigger TEXT,
    causal_consequence TEXT,
    created_at TEXT NOT NULL,
    embedding_model TEXT,
    embedding_version TEXT,
    supersedes TEXT,
    superseded_by TEXT,
    consolidated INTEGER DEFAULT 0,
    FOREIGN KEY (supersedes) REFERENCES episodes(id)
  );

  CREATE TABLE IF NOT EXISTS semantics (
    id TEXT PRIMARY KEY,
    content TEXT NOT NULL,
    embedding BLOB,
    state TEXT DEFAULT 'active' CHECK(state IN ('active','disputed','superseded','context_dependent','dormant','rolled_back')),
    conditions TEXT,
    evidence_episode_ids TEXT,
    evidence_count INTEGER DEFAULT 0,
    supporting_count INTEGER DEFAULT 0,
    contradicting_count INTEGER DEFAULT 0,
    source_type_diversity INTEGER DEFAULT 0,
    consolidation_checkpoint TEXT,
    embedding_model TEXT,
    embedding_version TEXT,
    consolidation_model TEXT,
    consolidation_prompt_hash TEXT,
    created_at TEXT NOT NULL,
    last_reinforced_at TEXT,
    retrieval_count INTEGER DEFAULT 0,
    challenge_count INTEGER DEFAULT 0
  );

  CREATE TABLE IF NOT EXISTS procedures (
    id TEXT PRIMARY KEY,
    content TEXT NOT NULL,
    embedding BLOB,
    state TEXT DEFAULT 'active' CHECK(state IN ('active','disputed','superseded','context_dependent','dormant','rolled_back')),
    trigger_conditions TEXT,
    evidence_episode_ids TEXT,
    success_count INTEGER DEFAULT 0,
    failure_count INTEGER DEFAULT 0,
    embedding_model TEXT,
    embedding_version TEXT,
    created_at TEXT NOT NULL,
    last_reinforced_at TEXT,
    retrieval_count INTEGER DEFAULT 0
  );

  CREATE TABLE IF NOT EXISTS causal_links (
    id TEXT PRIMARY KEY,
    cause_id TEXT NOT NULL,
    effect_id TEXT NOT NULL,
    link_type TEXT DEFAULT 'causal' CHECK(link_type IN ('causal','correlational','temporal')),
    mechanism TEXT,
    confidence REAL,
    evidence_count INTEGER DEFAULT 1,
    created_at TEXT NOT NULL
  );

  CREATE TABLE IF NOT EXISTS contradictions (
    id TEXT PRIMARY KEY,
    claim_a_id TEXT NOT NULL,
    claim_b_id TEXT NOT NULL,
    claim_a_type TEXT NOT NULL,
    claim_b_type TEXT NOT NULL,
    state TEXT DEFAULT 'open' CHECK(state IN ('open','resolved','context_dependent','reopened')),
    resolution TEXT,
    resolved_at TEXT,
    reopened_at TEXT,
    reopen_evidence_id TEXT,
    created_at TEXT NOT NULL
  );

  CREATE TABLE IF NOT EXISTS consolidation_runs (
    id TEXT PRIMARY KEY,
    checkpoint_cursor TEXT,
    input_episode_ids TEXT,
    output_memory_ids TEXT,
    confidence_deltas TEXT,
    consolidation_model TEXT,
    consolidation_prompt_hash TEXT,
    started_at TEXT,
    completed_at TEXT,
    status TEXT DEFAULT 'running' CHECK(status IN ('running','completed','failed','rolled_back'))
  );

  CREATE TABLE IF NOT EXISTS audrey_config (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL
  );

  CREATE INDEX IF NOT EXISTS idx_episodes_created ON episodes(created_at);
  CREATE INDEX IF NOT EXISTS idx_episodes_consolidated ON episodes(consolidated);
  CREATE INDEX IF NOT EXISTS idx_episodes_source ON episodes(source);
  CREATE INDEX IF NOT EXISTS idx_semantics_state ON semantics(state);
  CREATE INDEX IF NOT EXISTS idx_procedures_state ON procedures(state);
  CREATE INDEX IF NOT EXISTS idx_contradictions_state ON contradictions(state);
  CREATE INDEX IF NOT EXISTS idx_consolidation_status ON consolidation_runs(status);
`;

function createVecTables(db, dimensions) {
  db.exec(`
    CREATE VIRTUAL TABLE IF NOT EXISTS vec_episodes USING vec0(
      id text primary key,
      embedding float[${dimensions}] distance_metric=cosine,
      source text,
      consolidated integer
    );
    CREATE VIRTUAL TABLE IF NOT EXISTS vec_semantics USING vec0(
      id text primary key,
      embedding float[${dimensions}] distance_metric=cosine,
      state text
    );
    CREATE VIRTUAL TABLE IF NOT EXISTS vec_procedures USING vec0(
      id text primary key,
      embedding float[${dimensions}] distance_metric=cosine,
      state text
    );
  `);
}

function migrateEmbeddings(db) {
  const episodeCount = db.prepare(
    "SELECT COUNT(*) as c FROM episodes WHERE embedding IS NOT NULL"
  ).get().c;
  const vecCount = db.prepare(
    "SELECT COUNT(*) as c FROM vec_episodes"
  ).get().c;

  if (episodeCount > 0 && vecCount === 0) {
    db.prepare(`
      INSERT INTO vec_episodes(id, embedding, source, consolidated)
      SELECT id, embedding, source, consolidated FROM episodes WHERE embedding IS NOT NULL
    `).run();
  }

  const semCount = db.prepare(
    "SELECT COUNT(*) as c FROM semantics WHERE embedding IS NOT NULL"
  ).get().c;
  const vecSemCount = db.prepare(
    "SELECT COUNT(*) as c FROM vec_semantics"
  ).get().c;

  if (semCount > 0 && vecSemCount === 0) {
    db.prepare(`
      INSERT INTO vec_semantics(id, embedding, state)
      SELECT id, embedding, state FROM semantics WHERE embedding IS NOT NULL
    `).run();
  }

  const procCount = db.prepare(
    "SELECT COUNT(*) as c FROM procedures WHERE embedding IS NOT NULL"
  ).get().c;
  const vecProcCount = db.prepare(
    "SELECT COUNT(*) as c FROM vec_procedures"
  ).get().c;

  if (procCount > 0 && vecProcCount === 0) {
    db.prepare(`
      INSERT INTO vec_procedures(id, embedding, state)
      SELECT id, embedding, state FROM procedures WHERE embedding IS NOT NULL
    `).run();
  }
}

export function createDatabase(dataDir, options = {}) {
  const { dimensions } = options;
  mkdirSync(dataDir, { recursive: true });
  const dbPath = join(dataDir, 'audrey.db');
  const db = new Database(dbPath);
  db.pragma('journal_mode = WAL');
  db.pragma('foreign_keys = ON');
  db.pragma('busy_timeout = 5000');
  db.exec(SCHEMA);

  if (dimensions) {
    sqliteVec.load(db);

    const stored = db.prepare("SELECT value FROM audrey_config WHERE key = 'dimensions'").get();
    if (stored) {
      if (parseInt(stored.value) !== dimensions) {
        throw new Error(
          `Embedding dimensions mismatch: database has ${stored.value}, provider has ${dimensions}. ` +
          `Use embedding migration to re-index with new dimensions.`
        );
      }
    } else {
      db.prepare("INSERT INTO audrey_config (key, value) VALUES ('dimensions', ?)").run(String(dimensions));
    }

    createVecTables(db, dimensions);
    migrateEmbeddings(db);
  }

  return db;
}

export function closeDatabase(db) {
  if (db && db.open) {
    db.close();
  }
}
```

**Step 4: Run tests to verify they pass**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/vec.test.js`
Expected: All tests PASS

**Step 5: Run full test suite to verify no regressions**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All existing tests PASS (createDatabase without dimensions still works)

**Step 6: Commit**

```bash
cd A:/ai/claude/audrey
git add src/db.js tests/vec.test.js
git commit -m "feat: add sqlite-vec foundation with vec0 tables and migration"
```

---

## Task 2: Batch Embedding on Providers

**Files:**
- Modify: `src/embedding.js`
- Modify: `tests/embedding.test.js`

**Step 1: Write the failing tests**

Append to `tests/embedding.test.js`:

```js
// Add after existing MockEmbeddingProvider tests:

describe('MockEmbeddingProvider.embedBatch', () => {
  it('embeds multiple texts in one call', async () => {
    const provider = new MockEmbeddingProvider({ dimensions: 8 });
    const vectors = await provider.embedBatch(['hello', 'world', 'test']);
    expect(vectors.length).toBe(3);
    expect(vectors[0].length).toBe(8);
    // Same text produces same vector
    const single = await provider.embed('hello');
    expect(vectors[0]).toEqual(single);
  });
});

// Add after existing OpenAIEmbeddingProvider tests:

describe('OpenAIEmbeddingProvider.embedBatch', () => {
  it('sends batch request to OpenAI', async () => {
    const mockResponse = {
      data: [
        { embedding: [0.1, 0.2] },
        { embedding: [0.3, 0.4] },
      ],
    };
    global.fetch = vi.fn().mockResolvedValue({
      ok: true,
      json: () => Promise.resolve(mockResponse),
    });

    const provider = new OpenAIEmbeddingProvider({ apiKey: 'test-key', dimensions: 2 });
    const vectors = await provider.embedBatch(['text1', 'text2']);

    expect(vectors).toEqual([[0.1, 0.2], [0.3, 0.4]]);
    const body = JSON.parse(global.fetch.mock.calls[0][1].body);
    expect(body.input).toEqual(['text1', 'text2']);
  });
});
```

**Step 2: Run to verify they fail**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/embedding.test.js`
Expected: FAIL — `embedBatch is not a function`

**Step 3: Add embedBatch to both providers**

Add to `MockEmbeddingProvider`:

```js
async embedBatch(texts) {
  return Promise.all(texts.map(t => this.embed(t)));
}
```

Add to `OpenAIEmbeddingProvider`:

```js
async embedBatch(texts) {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ input: texts, model: this.model, dimensions: this.dimensions }),
  });
  if (!response.ok) throw new Error(`OpenAI embedding failed: ${response.status}`);
  const data = await response.json();
  return data.data.map(d => d.embedding);
}
```

**Step 4: Run tests to verify they pass**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/embedding.test.js`
Expected: All tests PASS

**Step 5: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 6: Commit**

```bash
cd A:/ai/claude/audrey
git add src/embedding.js tests/embedding.test.js
git commit -m "feat: add embedBatch to embedding providers"
```

---

## Task 3: Encode Rewrite — vec0 Primary Write Path

**Files:**
- Modify: `src/encode.js`
- Modify: `tests/encode.test.js`

**Step 1: Read existing encode.js and encode.test.js**

Understand current write path before changing.

**Step 2: Update `tests/encode.test.js`**

All `createDatabase(TEST_DIR)` calls in encode.test.js become `createDatabase(TEST_DIR, { dimensions: 8 })`.

Add new tests:

```js
it('writes embedding to vec_episodes table', async () => {
  const id = await encodeEpisode(db, embedding, {
    content: 'test observation',
    source: 'direct-observation',
  });
  const vecRow = db.prepare('SELECT id FROM vec_episodes WHERE id = ?').get(id);
  expect(vecRow).toBeDefined();
  expect(vecRow.id).toBe(id);
});

it('writes source and consolidated metadata to vec_episodes', async () => {
  const id = await encodeEpisode(db, embedding, {
    content: 'test',
    source: 'tool-result',
  });
  // Query vec_episodes via KNN to verify metadata was written
  const vec = db.prepare('SELECT embedding FROM vec_episodes WHERE id = ?').get(id);
  expect(vec).toBeDefined();
});
```

**Step 3: Modify `src/encode.js`**

Change the INSERT to:
1. Insert into `episodes` WITHOUT embedding (pass NULL for embedding column)
2. Insert into `vec_episodes` with embedding + source + consolidated=0

Check if vec_episodes exists before writing to it (graceful for databases opened without dimensions).

```js
// After inserting into episodes:
const hasVec = db.prepare(
  "SELECT 1 FROM sqlite_master WHERE type='table' AND name='vec_episodes'"
).get();

if (hasVec) {
  db.prepare(
    'INSERT INTO vec_episodes(id, embedding, source, consolidated) VALUES (?, ?, ?, 0)'
  ).run(id, embeddingBuffer, params.source);
}
```

**Step 4: Run tests**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/encode.test.js`
Expected: All tests PASS

**Step 5: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 6: Commit**

```bash
cd A:/ai/claude/audrey
git add src/encode.js tests/encode.test.js
git commit -m "feat: encode writes vectors to vec_episodes via sqlite-vec"
```

---

## Task 4: Recall Rewrite — KNN + Streaming

**Files:**
- Modify: `src/recall.js`
- Modify: `tests/recall.test.js`

**Step 1: Update `tests/recall.test.js`**

Update all `createDatabase(TEST_DIR)` → `createDatabase(TEST_DIR, { dimensions: 8 })`.

Update any test that manually inserts embeddings into the main tables to also insert into vec tables. Pattern:

```js
// Old: just insert into semantics with embedding
db.prepare(`INSERT INTO semantics (..., embedding, ...) VALUES (...)`).run(..., vecBuf, ...);

// New: insert into semantics (no embedding needed) + vec_semantics
db.prepare(`INSERT INTO semantics (id, content, state, ...) VALUES (...)`).run(...);
db.prepare(`INSERT INTO vec_semantics (id, embedding, state) VALUES (?, ?, ?)`).run(id, vecBuf, 'active');
```

Add streaming recall test:

```js
import { recall, recallStream } from '../src/recall.js';

describe('recallStream', () => {
  it('yields results as async generator', async () => {
    // ... setup episodes ...
    const results = [];
    for await (const memory of recallStream(db, embedding, 'test query', { limit: 5 })) {
      results.push(memory);
    }
    expect(results.length).toBeGreaterThan(0);
    expect(results[0]).toHaveProperty('score');
  });

  it('supports early break', async () => {
    // ... setup multiple episodes ...
    let count = 0;
    for await (const memory of recallStream(db, embedding, 'test', { limit: 10 })) {
      count++;
      if (count >= 2) break;
    }
    expect(count).toBe(2);
  });
});
```

**Step 2: Rewrite `src/recall.js`**

Replace the full-scan approach with KNN queries:

```js
import { computeConfidence, DEFAULT_HALF_LIVES } from './confidence.js';
import { daysBetween, safeJsonParse } from './utils.js';

const CANDIDATE_MULTIPLIER = 3;

function hasVecTable(db, tableName) {
  return !!db.prepare(
    "SELECT 1 FROM sqlite_master WHERE type='table' AND name=?"
  ).get(tableName);
}

export async function* recallStream(db, embeddingProvider, query, options = {}) {
  const {
    minConfidence = 0,
    types,
    limit = 10,
    includeProvenance = false,
    includeDormant = false,
  } = options;

  const queryVector = await embeddingProvider.embed(query);
  const queryBuffer = embeddingProvider.vectorToBuffer(queryVector);
  const searchTypes = types || ['episodic', 'semantic', 'procedural'];
  const now = new Date();
  const candidateK = limit * CANDIDATE_MULTIPLIER;
  const useVec = hasVecTable(db, 'vec_episodes');
  const scored = [];

  if (searchTypes.includes('episodic')) {
    let candidates;
    if (useVec) {
      candidates = db.prepare(`
        SELECT e.*, (1.0 - v.distance) AS similarity
        FROM vec_episodes v
        JOIN episodes e ON e.id = v.id
        WHERE v.embedding MATCH ?
          AND k = ?
          AND e.superseded_by IS NULL
      `).all(queryBuffer, candidateK);
    } else {
      // Fallback for databases without vec0 (e.g. legacy tests)
      const { cosineSimilarity } = await import('./utils.js');
      const all = db.prepare(
        'SELECT * FROM episodes WHERE superseded_by IS NULL AND embedding IS NOT NULL'
      ).all();
      candidates = all.map(ep => ({
        ...ep,
        similarity: cosineSimilarity(queryBuffer, ep.embedding, embeddingProvider),
      }));
    }

    for (const ep of candidates) {
      const ageDays = daysBetween(ep.created_at, now);
      const confidence = computeConfidence({
        sourceType: ep.source,
        supportingCount: 1,
        contradictingCount: 0,
        ageDays,
        halfLifeDays: DEFAULT_HALF_LIVES.episodic,
        retrievalCount: 0,
        daysSinceRetrieval: ageDays,
      });
      if (confidence < minConfidence) continue;

      const entry = {
        id: ep.id,
        content: ep.content,
        type: 'episodic',
        confidence,
        score: ep.similarity * confidence,
        source: ep.source,
        createdAt: ep.created_at,
      };
      if (includeProvenance) {
        entry.provenance = {
          source: ep.source,
          sourceReliability: ep.source_reliability,
          createdAt: ep.created_at,
          supersedes: ep.supersedes || null,
        };
      }
      scored.push(entry);
    }
  }

  if (searchTypes.includes('semantic')) {
    const stateList = includeDormant
      ? "'active','context_dependent','dormant'"
      : "'active','context_dependent'";

    let candidates;
    if (useVec) {
      candidates = db.prepare(`
        SELECT s.*, (1.0 - v.distance) AS similarity
        FROM vec_semantics v
        JOIN semantics s ON s.id = v.id
        WHERE v.embedding MATCH ?
          AND k = ?
          AND v.state IN (${stateList})
      `).all(queryBuffer, candidateK);
    } else {
      const { cosineSimilarity } = await import('./utils.js');
      const all = db.prepare(
        `SELECT * FROM semantics WHERE state IN (${stateList}) AND embedding IS NOT NULL`
      ).all();
      candidates = all.map(s => ({
        ...s,
        similarity: cosineSimilarity(queryBuffer, s.embedding, embeddingProvider),
      }));
    }

    const matchedIds = [];
    for (const sem of candidates) {
      const ageDays = daysBetween(sem.created_at, now);
      const daysSinceRetrieval = sem.last_reinforced_at
        ? daysBetween(sem.last_reinforced_at, now) : ageDays;
      const confidence = computeConfidence({
        sourceType: 'tool-result',
        supportingCount: sem.supporting_count || 0,
        contradictingCount: sem.contradicting_count || 0,
        ageDays,
        halfLifeDays: DEFAULT_HALF_LIVES.semantic,
        retrievalCount: sem.retrieval_count || 0,
        daysSinceRetrieval,
      });
      if (confidence < minConfidence) continue;
      matchedIds.push(sem.id);

      const entry = {
        id: sem.id,
        content: sem.content,
        type: 'semantic',
        confidence,
        score: sem.similarity * confidence,
        source: 'consolidation',
        state: sem.state,
        createdAt: sem.created_at,
      };
      if (includeProvenance) {
        entry.provenance = {
          evidenceEpisodeIds: safeJsonParse(sem.evidence_episode_ids, []),
          evidenceCount: sem.evidence_count || 0,
          supportingCount: sem.supporting_count || 0,
          contradictingCount: sem.contradicting_count || 0,
          consolidationCheckpoint: sem.consolidation_checkpoint || null,
        };
      }
      scored.push(entry);
    }

    if (matchedIds.length > 0) {
      const updateStmt = db.prepare(
        'UPDATE semantics SET retrieval_count = retrieval_count + 1, last_reinforced_at = ? WHERE id = ?'
      );
      const nowISO = now.toISOString();
      for (const id of matchedIds) updateStmt.run(nowISO, id);
    }
  }

  if (searchTypes.includes('procedural')) {
    const stateList = includeDormant
      ? "'active','context_dependent','dormant'"
      : "'active','context_dependent'";

    let candidates;
    if (useVec) {
      candidates = db.prepare(`
        SELECT p.*, (1.0 - v.distance) AS similarity
        FROM vec_procedures v
        JOIN procedures p ON p.id = v.id
        WHERE v.embedding MATCH ?
          AND k = ?
          AND v.state IN (${stateList})
      `).all(queryBuffer, candidateK);
    } else {
      const { cosineSimilarity } = await import('./utils.js');
      const all = db.prepare(
        `SELECT * FROM procedures WHERE state IN (${stateList}) AND embedding IS NOT NULL`
      ).all();
      candidates = all.map(p => ({
        ...p,
        similarity: cosineSimilarity(queryBuffer, p.embedding, embeddingProvider),
      }));
    }

    const matchedIds = [];
    for (const proc of candidates) {
      const ageDays = daysBetween(proc.created_at, now);
      const daysSinceRetrieval = proc.last_reinforced_at
        ? daysBetween(proc.last_reinforced_at, now) : ageDays;
      const confidence = computeConfidence({
        sourceType: 'tool-result',
        supportingCount: proc.success_count || 0,
        contradictingCount: proc.failure_count || 0,
        ageDays,
        halfLifeDays: DEFAULT_HALF_LIVES.procedural,
        retrievalCount: proc.retrieval_count || 0,
        daysSinceRetrieval,
      });
      if (confidence < minConfidence) continue;
      matchedIds.push(proc.id);

      const entry = {
        id: proc.id,
        content: proc.content,
        type: 'procedural',
        confidence,
        score: proc.similarity * confidence,
        source: 'consolidation',
        state: proc.state,
        createdAt: proc.created_at,
      };
      if (includeProvenance) {
        entry.provenance = {
          evidenceEpisodeIds: safeJsonParse(proc.evidence_episode_ids, []),
          successCount: proc.success_count || 0,
          failureCount: proc.failure_count || 0,
          triggerConditions: proc.trigger_conditions || null,
        };
      }
      scored.push(entry);
    }

    if (matchedIds.length > 0) {
      const updateStmt = db.prepare(
        'UPDATE procedures SET retrieval_count = retrieval_count + 1, last_reinforced_at = ? WHERE id = ?'
      );
      const nowISO = now.toISOString();
      for (const id of matchedIds) updateStmt.run(nowISO, id);
    }
  }

  scored.sort((a, b) => b.score - a.score);
  const top = scored.slice(0, limit);
  for (const entry of top) {
    yield entry;
  }
}

export async function recall(db, embeddingProvider, query, options = {}) {
  const results = [];
  for await (const entry of recallStream(db, embeddingProvider, query, options)) {
    results.push(entry);
  }
  return results;
}
```

**Step 3: Run tests**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/recall.test.js`
Expected: All tests PASS

**Step 4: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 5: Commit**

```bash
cd A:/ai/claude/audrey
git add src/recall.js tests/recall.test.js
git commit -m "feat: rewrite recall with sqlite-vec KNN and streaming async generator"
```

---

## Task 5: Validate Rewrite — KNN k=1

**Files:**
- Modify: `src/validate.js`
- Modify: `tests/validate.test.js`

**Step 1: Update `tests/validate.test.js`**

Update `createDatabase(TEST_DIR)` → `createDatabase(TEST_DIR, { dimensions: 8 })`.

Update direct embedding inserts to also write to `vec_semantics`.

**Step 2: Modify `src/validate.js`**

Replace the full-scan embedding comparison with a KNN k=1 query:

```js
// Instead of scanning all semantics in JS:
const hasVec = !!db.prepare(
  "SELECT 1 FROM sqlite_master WHERE type='table' AND name='vec_semantics'"
).get();

let bestMatch = null;
let bestSimilarity = 0;

if (hasVec) {
  const result = db.prepare(`
    SELECT s.*, (1.0 - v.distance) AS similarity
    FROM vec_semantics v
    JOIN semantics s ON s.id = v.id
    WHERE v.embedding MATCH ?
      AND k = 1
      AND v.state IN ('active', 'context_dependent')
  `).get(episodeBuffer);

  if (result) {
    bestMatch = result;
    bestSimilarity = result.similarity;
  }
} else {
  // Legacy fallback
  // ... existing brute-force code ...
}
```

For the episode vector: encode.js now writes to vec_episodes, so the vector is available there. But validateMemory receives the episode content and needs to embed it. The current code already embeds it:

```js
const episodeVector = await embeddingProvider.embed(episode.content);
const episodeBuffer = embeddingProvider.vectorToBuffer(episodeVector);
```

This buffer is used directly in the MATCH query.

**Step 3: Run tests**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/validate.test.js`
Expected: All tests PASS

**Step 4: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 5: Commit**

```bash
cd A:/ai/claude/audrey
git add src/validate.js tests/validate.test.js
git commit -m "feat: rewrite validation with sqlite-vec KNN k=1"
```

---

## Task 6: Consolidation Clustering Rewrite

**Files:**
- Modify: `src/consolidate.js`
- Modify: `tests/consolidate.test.js`

**Step 1: Update `tests/consolidate.test.js`**

Update `createDatabase(TEST_DIR)` → `createDatabase(TEST_DIR, { dimensions: 8 })`.

**Step 2: Rewrite `clusterEpisodes` in `src/consolidate.js`**

Replace O(N²) JS pairwise cosine with N KNN queries:

```js
export function clusterEpisodes(db, embeddingProvider, options = {}) {
  const { similarityThreshold = 0.85, minClusterSize = 3 } = options;

  const episodes = db.prepare(
    'SELECT id FROM episodes WHERE consolidated = 0 AND superseded_by IS NULL'
  ).all();

  if (episodes.length === 0) return [];

  const n = episodes.length;
  const idToIdx = new Map(episodes.map((ep, i) => [ep.id, i]));
  const parent = new Array(n);
  for (let i = 0; i < n; i++) parent[i] = i;

  function find(x) {
    while (parent[x] !== x) { parent[x] = parent[parent[x]]; x = parent[x]; }
    return x;
  }
  function union(a, b) {
    const ra = find(a), rb = find(b);
    if (ra !== rb) parent[ra] = rb;
  }

  const hasVec = !!db.prepare(
    "SELECT 1 FROM sqlite_master WHERE type='table' AND name='vec_episodes'"
  ).get();

  if (hasVec) {
    const knnStmt = db.prepare(`
      SELECT id, (1.0 - distance) AS similarity
      FROM vec_episodes
      WHERE embedding MATCH (SELECT embedding FROM vec_episodes WHERE id = ?)
        AND k = ?
        AND consolidated = 0
    `);

    for (const ep of episodes) {
      const neighbors = knnStmt.all(ep.id, Math.min(50, n));
      for (const neighbor of neighbors) {
        if (neighbor.id === ep.id) continue;
        if (neighbor.similarity >= similarityThreshold) {
          const i = idToIdx.get(ep.id);
          const j = idToIdx.get(neighbor.id);
          if (i !== undefined && j !== undefined) union(i, j);
        }
      }
    }
  } else {
    // Legacy fallback for databases without vec0
    const { cosineSimilarity } = await import('./utils.js');
    const fullEpisodes = db.prepare(
      'SELECT * FROM episodes WHERE consolidated = 0 AND superseded_by IS NULL AND embedding IS NOT NULL'
    ).all();
    for (let i = 0; i < fullEpisodes.length; i++) {
      for (let j = i + 1; j < fullEpisodes.length; j++) {
        const sim = cosineSimilarity(fullEpisodes[i].embedding, fullEpisodes[j].embedding, embeddingProvider);
        if (sim >= similarityThreshold) union(i, j);
      }
    }
  }

  // Group by root
  const groups = new Map();
  for (let i = 0; i < n; i++) {
    const root = find(i);
    if (!groups.has(root)) groups.set(root, []);
    groups.get(root).push(episodes[i].id);
  }

  // Fetch full episode data for clusters meeting minClusterSize
  const clusters = [];
  for (const ids of groups.values()) {
    if (ids.length >= minClusterSize) {
      const placeholders = ids.map(() => '?').join(',');
      const rows = db.prepare(
        `SELECT * FROM episodes WHERE id IN (${placeholders})`
      ).all(...ids);
      clusters.push(rows);
    }
  }

  return clusters;
}
```

Note: `clusterEpisodes` becomes sync when using vec0 (no async embed calls needed). But the legacy fallback uses dynamic import. Since `runConsolidation` is already async, this works fine.

Also update the semantic write in `runConsolidation` to write to `vec_semantics`:

```js
// After inserting into semantics table, also insert into vec_semantics
const hasVecSem = !!db.prepare(
  "SELECT 1 FROM sqlite_master WHERE type='table' AND name='vec_semantics'"
).get();

if (hasVecSem) {
  db.prepare(
    'INSERT INTO vec_semantics(id, embedding, state) VALUES (?, ?, ?)'
  ).run(data.semanticId, data.embeddingBuffer, 'active');
}
```

**Step 3: Run tests**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/consolidate.test.js`
Expected: All tests PASS

**Step 4: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 5: Commit**

```bash
cd A:/ai/claude/audrey
git add src/consolidate.js tests/consolidate.test.js
git commit -m "feat: rewrite clustering with N KNN queries via sqlite-vec"
```

---

## Task 7: Audrey Class — encodeBatch, recallStream, Dimensions

**Files:**
- Modify: `src/audrey.js`
- Modify: `src/index.js`
- Modify: `tests/audrey.test.js`

**Step 1: Add failing tests**

Append to `tests/audrey.test.js`:

```js
describe('Audrey batch and streaming', () => {
  let brain;

  beforeEach(() => {
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
    brain = new Audrey({
      dataDir: TEST_DIR,
      agent: 'test-agent',
      embedding: { provider: 'mock', dimensions: 8 },
    });
  });

  afterEach(() => {
    brain.close();
    if (existsSync(TEST_DIR)) rmSync(TEST_DIR, { recursive: true });
  });

  it('encodeBatch encodes multiple episodes in one call', async () => {
    const ids = await brain.encodeBatch([
      { content: 'Observation A', source: 'direct-observation' },
      { content: 'Observation B', source: 'tool-result' },
      { content: 'Observation C', source: 'told-by-user' },
    ]);
    expect(ids.length).toBe(3);
    const count = brain.db.prepare('SELECT COUNT(*) as c FROM episodes').get().c;
    expect(count).toBe(3);
  });

  it('recallStream yields results as async generator', async () => {
    await brain.encode({ content: 'Test memory', source: 'direct-observation' });
    const results = [];
    for await (const memory of brain.recallStream('test', { limit: 5 })) {
      results.push(memory);
    }
    expect(results.length).toBeGreaterThan(0);
  });
});
```

**Step 2: Modify `src/audrey.js`**

Pass dimensions to createDatabase:

```js
this.db = createDatabase(dataDir, { dimensions: this.embeddingProvider.dimensions });
```

Add `encodeBatch`:

```js
async encodeBatch(paramsList) {
  const contents = paramsList.map(p => {
    if (!p.content || typeof p.content !== 'string') {
      throw new Error('content must be a non-empty string');
    }
    return p.content;
  });

  const vectors = await this.embeddingProvider.embedBatch(contents);
  const ids = [];

  const insertAll = this.db.transaction(() => {
    for (let i = 0; i < paramsList.length; i++) {
      const id = encodeEpisode.sync
        ? encodeEpisode(this.db, this.embeddingProvider, paramsList[i], vectors[i])
        : null;
      // We need a sync version or pre-computed vectors
    }
  });

  // Actually, since encodeEpisode is async (calls embed), we need a different approach.
  // Pre-compute all vectors, then do sync inserts in a transaction.
  // This requires a lower-level insert function.
  // ... (implementation detail to work out during execution)

  for (let i = 0; i < paramsList.length; i++) {
    const id = await encodeEpisode(this.db, this.embeddingProvider, paramsList[i]);
    ids.push(id);
    this.emit('encode', { id, ...paramsList[i] });
  }
  return ids;
}
```

Note: The actual implementation will need `encodeEpisode` to accept pre-computed vectors, or we extract the insert logic. The executing engineer should solve this by adding an optional `precomputedVector` param to `encodeEpisode`.

Add `recallStream`:

```js
async *recallStream(query, options = {}) {
  yield* recallStreamFn(this.db, this.embeddingProvider, query, options);
}
```

Update `src/index.js` to export `recallStream`.

**Step 3: Run tests**

Run: `cd A:/ai/claude/audrey && npx vitest run tests/audrey.test.js`
Expected: All tests PASS

**Step 4: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 5: Commit**

```bash
cd A:/ai/claude/audrey
git add src/audrey.js src/encode.js src/index.js tests/audrey.test.js
git commit -m "feat: add encodeBatch, recallStream, pass dimensions to database"
```

---

## Task 8: Update Remaining Tests + Cleanup

**Files:**
- Modify: `tests/rollback.test.js` — add `{ dimensions: 8 }` to createDatabase
- Modify: `tests/decay.test.js` — add `{ dimensions: 8 }`, update direct embedding inserts
- Modify: `tests/introspect.test.js` — add `{ dimensions: 8 }` to createDatabase
- Modify: `tests/causal.test.js` — add `{ dimensions: 8 }` to createDatabase
- Modify: `tests/validate.test.js` — ensure vec_semantics inserts in manual setup
- Modify: `src/utils.js` — keep `cosineSimilarity` for now (legacy fallback), mark deprecated

**Step 1: Update each test file's createDatabase calls**

Pattern: `createDatabase(TEST_DIR)` → `createDatabase(TEST_DIR, { dimensions: 8 })`

For tests that manually insert embeddings into semantics/procedures, also insert into the corresponding vec table.

**Step 2: Run full suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 3: Commit**

```bash
cd A:/ai/claude/audrey
git add tests/
git commit -m "chore: update all tests for sqlite-vec vec0 tables"
```

---

## Task 9: Version Bump + Final Verification

**Files:**
- Modify: `package.json` — `"version": "0.3.0"`

**Step 1: Bump version**

Change `"version": "0.2.0"` to `"version": "0.3.0"`.

**Step 2: Run full test suite**

Run: `cd A:/ai/claude/audrey && npm test`
Expected: All tests PASS

**Step 3: Commit**

```bash
cd A:/ai/claude/audrey
git add package.json
git commit -m "chore: bump version to 0.3.0"
```

---

## Summary of File Changes

| Task | File | Action |
|------|------|--------|
| 1 | `src/db.js` | Modify — sqlite-vec loading, vec0 tables, config, migration |
| 1 | `tests/vec.test.js` | Create — sqlite-vec integration tests |
| 2 | `src/embedding.js` | Modify — add embedBatch |
| 2 | `tests/embedding.test.js` | Modify — embedBatch tests |
| 3 | `src/encode.js` | Modify — write to vec_episodes |
| 3 | `tests/encode.test.js` | Modify — verify vec writes |
| 4 | `src/recall.js` | Modify — KNN queries + recallStream generator |
| 4 | `tests/recall.test.js` | Modify — KNN recall + streaming tests |
| 5 | `src/validate.js` | Modify — KNN k=1 for nearest match |
| 5 | `tests/validate.test.js` | Modify — vec_semantics setup |
| 6 | `src/consolidate.js` | Modify — N KNN clustering + vec_semantics writes |
| 6 | `tests/consolidate.test.js` | Modify — dimensions + vec setup |
| 7 | `src/audrey.js` | Modify — encodeBatch, recallStream, dimensions |
| 7 | `src/index.js` | Modify — export recallStream |
| 7 | `tests/audrey.test.js` | Modify — batch + streaming tests |
| 8 | `tests/*.test.js` | Modify — dimensions on all createDatabase calls |
| 9 | `package.json` | Modify — version 0.3.0 |
